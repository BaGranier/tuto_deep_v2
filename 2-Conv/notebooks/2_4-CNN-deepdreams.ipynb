{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511188c0",
   "metadata": {},
   "source": [
    "# Génération d'image & jeu avec le gradient\n",
    "\n",
    "L'idée est de récupérer une architecture pré-entrainée et d'aller jouer avec. Deux options assez simples sont envisageables:\n",
    "1. Les approches *deepdream* : [github](https://github.com/google/deepdream/tree/master); [LeMonde](https://www.lemonde.fr/pixels/article/2015/07/09/on-a-teste-pour-vous-deep-dream-la-machine-a-reves-psychedeliques-de-google_4675562_4408996.html)\n",
    "1. Les approches *style/content*, dites aussi transfert de style [github](https://github.com/Raed-Alshehri/Art_Generation_Neural_Style_Transfer). Si j'ai une image A qui m'interesse pour son contenu et une image B (i.e. un Van Gogh ou Munch) qui m'interesse pour son style, suis-je capable de construire une version A' qui prend le style de B?\n",
    "\n",
    "Dans tous les cas, il faut récupérer un modèle (par exemple GoogLeNet) pré-entrainé puis récupérer et contraindre des couches intermédiaires (=définir une loss) puis ensuite activer le gradient sur l'entrée et laisser dériver l'image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation d'un googlenet\n",
    "# affichage des différentes couches => ce sera critique pour venir s'accrocher sur le réseau\n",
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd6858",
   "metadata": {},
   "source": [
    "## A. Classification d'image\n",
    "\n",
    "Vérifier que le réseau marche bien en classification d'image\n",
    "\n",
    "1. Charger une image\n",
    "1. Appliquer le classifieur\n",
    "1. Télécharger la signification des étiquettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07345ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"img/dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a93cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# load image\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68223fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68da0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2715746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e02515",
   "metadata": {},
   "source": [
    "## B. Deepdream\n",
    "\n",
    "1. Charger l'image + activer le gradient\n",
    "1. La passer dans le réseau et récupérer une couche cachée\n",
    "1. Définir une loss sur cette couche\n",
    "1. Rétro-propager l'erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fec8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# --- 2. Charger une image PIL et la prétraiter ---\n",
    "def load_image(path, max_size=500):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(max_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "img = load_image(\"img/dog.jpg\")\n",
    "img_orig = copy.deepcopy(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351dc61",
   "metadata": {},
   "source": [
    "On va utiliser les *hooks* = récupération très simplifiée d'information à l'intérieur du réseau\n",
    "1. On *pose* le crochet\n",
    "1. On fait le forward\n",
    "1. On a l'information dans une structure de données, on va pouvoir créer notre loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Choisir une couche cible ---\n",
    "layer_name = 'inception4b'  # peut être 'inception3a', 'inception4d', 'inception4c' etc.\n",
    "\n",
    "# --- 4. Extraire les activations intermédiaires ---\n",
    "activations = {}\n",
    "\n",
    "# Crochet\n",
    "def hook_fn(module, input, output):\n",
    "    activations['feat'] = output\n",
    "\n",
    "# Récupérer le module voulu\n",
    "target_layer = dict(model.named_modules())[layer_name]\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# --- 5. DeepDream : ascension de gradient sur l’image ---\n",
    "img.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([img], lr=0.1) # jouer avec le learning rate + nb itération\n",
    "\n",
    "for i in range(30):  # nombre d’itérations\n",
    "    optimizer.zero_grad()\n",
    "    model(img)\n",
    "    loss = -activations['feat'].norm()  # maximise la norme des activations ATTENTION à bien mettre un moins !\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optionnel : normaliser légèrement les pixels pour la stabilité\n",
    "    img.data = torch.clamp(img.data, -2, 2)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Iteration {i}, Loss = {loss.item():.2f}\")\n",
    "\n",
    "hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Convertir l’image en format affichable ---\n",
    "def deprocess(tensor):\n",
    "    tensor = tensor.detach().cpu().squeeze(0)\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return transforms.ToPILImage()(tensor)\n",
    "\n",
    "dream_img = deprocess(img)\n",
    "img_orig2  = deprocess(img_orig)\n",
    "dream_img_c = dream_img.crop((50, 50, 200, 200))\n",
    "img_orig_c = img_orig2.crop((50, 50, 200, 200))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(img_orig2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(dream_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(img_orig_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(dream_img_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6556c9b",
   "metadata": {},
   "source": [
    "## C. Reconstruction d'image\n",
    "\n",
    "1. A *encoder* (=forward) une image dans un réseau \n",
    "1. Isoler une représentation de cette image... Permettra-t-elle de reconstruire l'image?\n",
    "1. Partir d'une image aléatoire (bruit blanc gaussien) avec gradient activé\n",
    "1. Minimiser la distance au contenu encodé dans l'embedding\n",
    "\n",
    "On va travailler avec le même réseau. Mais on introduit différentes optimisation pour aller plus vite en calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR # pour accélérer\n",
    "\n",
    "\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "# pour les possesseurs de mac M1 avec la dernière version de pytorch:\n",
    "print(\"Le calcul GPU est disponible ? \", torch.backends.mps.is_available())\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154ad49",
   "metadata": {},
   "source": [
    "Le transfert de style sera à faire par vous même... Mais afin de simplifier les choses, je vous propose un code permettant la reconstruction d'une image à partir d'un embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = load_image(\"img/dog.jpg\").to(device)\n",
    "\n",
    "layer_content = 'inception3a'  # la première (bon encodage du contenu)\n",
    "activations = {}\n",
    "\n",
    "# Crochet\n",
    "def hook_fn(module, input, output):\n",
    "    activations['feat'] = output\n",
    "target_layer = dict(model.named_modules())[layer_content]\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "\n",
    "# target = une couche de l'encodage d'une image\n",
    "with torch.no_grad():\n",
    "    model(img)\n",
    "    target = activations['feat'].clone()\n",
    "    print(\"dimension de l'embedding : \", target.shape)\n",
    "\n",
    "\n",
    "# Générer une image aléatoire\n",
    "noise = torch.randn_like(img).to(device) #*0.1\n",
    "\n",
    "# --- 5. Generation d'image : minimiser la norme entre les embeddings ---\n",
    "noise.requires_grad_(True)\n",
    "\n",
    "nepoch = 1000\n",
    "max_lr=5e-1 # version optimisée OneCycle\n",
    "optimizer = torch.optim.AdamW([noise], lr=max_lr)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=1, epochs=nepoch)\n",
    "\n",
    "# optimizer = torch.optim.Adam([noise], lr=0.05) # jouer avec le learning rate + nb itération\n",
    "\n",
    "for i in range(nepoch):  # nombre d’itérations\n",
    "    optimizer.zero_grad()\n",
    "    model(noise)\n",
    "    loss = (target-activations['feat']).norm()  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Optionnel : normaliser légèrement les pixels pour la stabilité\n",
    "    noise.data = torch.clamp(noise.data, -2, 2)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Iteration {i}, Loss = {loss.item():.2f}\")\n",
    "\n",
    "    # if i % 200 == 0:  # Si vous voulez voir la progression\n",
    "    #     dream_img = deprocess(noise.to(\"cpu\"))\n",
    "    #     dream_img.save(\"save/image\"+str(i)+\".jpg\")\n",
    "\n",
    "hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Convertir l’image en format affichable ---\n",
    "\n",
    "dream_img = deprocess(noise.to(\"cpu\"))\n",
    "img_orig2  = deprocess(img.to(\"cpu\"))\n",
    "dream_img_c = dream_img.crop((50, 50, 200, 200))\n",
    "img_orig_c = img_orig2.crop((50, 50, 200, 200))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(img_orig2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(dream_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(img_orig_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(dream_img_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661e345",
   "metadata": {},
   "source": [
    "## D. Transfert de style\n",
    "\n",
    "Le transfert de style [lien](https://github.com/Raed-Alshehri/Art_Generation_Neural_Style_Transfer) consiste:\n",
    "1. A *encoder* (=forward) deux images dans un réseau \n",
    "1. Isoler deux représentations à deux étages différents (couche intermédiaire pour le contenu, couche profonde pour le style)\n",
    "1. Partir d'une image aléatoire (bruit blanc gaussien) avec gradient activé\n",
    "1. Minimiser la distance à la fois au contenu et au style (= 2 normes à pondérer)\n",
    "\n",
    "On va travailler avec le même réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033b7ed",
   "metadata": {},
   "source": [
    "A vous de jouer pour le transfert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = load_image(\"img/dog.jpg\").to(device)\n",
    "style = load_image(\"img/vg.jpg\").to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6069d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Choisir une couche cible ---\n",
    "layer_content = 'inception3a'  # la première\n",
    "layer_style   = 'inception5a'  # la dernière\n",
    "\n",
    "###  TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c03296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Convertir l’image en format affichable ---\n",
    "\n",
    "dream_img = deprocess(noise.to(\"cpu\"))\n",
    "img_orig2  = deprocess(img.to(\"cpu\"))\n",
    "dream_img_c = dream_img.crop((50, 50, 200, 200))\n",
    "img_orig_c = img_orig2.crop((50, 50, 200, 200))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(img_orig2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(dream_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(img_orig_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(dream_img_c)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee37f869",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769ea952",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad23364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
