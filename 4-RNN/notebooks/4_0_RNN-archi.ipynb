{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch\n",
    "\n",
    "Notebook normalement peu utile si vous avez eu le cours... Sinon, c'est une manière d'aborder d'intercaler une marche avant le premier notebook sur les séries temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RNN, device,SampleMetroDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from itertools import chain\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première idée: construire la cellule récurrente\n",
    "\n",
    "C'est ce qui est fait dans les notebooks des tuto officiels (remarquables) de pytorch:\n",
    "-   [NLP From Scratch: Classifying Names with a Character-Level\n",
    "    RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "-   [NLP From Scratch: Generating Names with a Character-Level\n",
    "    RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle la structure classique en classification de signaux (many-to-one):\n",
    "\n",
    "![Image RNN](data/rnn_unfold2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du réseau\n",
    "\n",
    "1. Réseau correspondant au besoin ci-dessus\n",
    "2. Données arbitraires\n",
    "3. Instanciation du réseau + utilisation sur un exemple\n",
    "4. Passage dans une loss pour vérifier la cohérence des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_V1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_V1, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size) # W2\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # W3\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden==None: hidden = self.initHidden() # init par défaut à 0\n",
    "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 2. Data\n",
    "\n",
    "# data (x_1,...,x_T), batch, dimension des entrées\n",
    "#   dim1 : Seq. length T=23 \n",
    "#   dim2 : batch of size 10\n",
    "#   dim3 : Input x_t \\in R^n dimension 100\n",
    "seq_x = torch.rand(23, 10, 100) # simulation d'un batch\n",
    "y     = torch.randint(0,2,(10,))\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_hidden = 50\n",
    "d_data   = 100\n",
    "n_cl     = 2\n",
    "rnn = RNN_V1(d_data, n_hidden, n_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire passer les données dans le réseau...\n",
    "\n",
    "Qu'est ce que j'ai le droit de donner à ce réseau??\n",
    "\n",
    "Avez-vous remarqué qu'il n'est pas récurrent??\n",
    "\n",
    "Sélectionner une propostion parmi les deux boites suivantes en réfléchissant aux conséquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([23, 10, 2])\n",
      "hidden:  torch.Size([23, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# PROPOSITION 1:\n",
    "yhat, hidden = rnn(seq_x)\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# PROPOSITION 2:\n",
    "yhat, hidden = rnn(seq_x[0,:,:])\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN et récurrence\n",
    "\n",
    "Evidemment, traiter une séquence entière sans prendre en compte les états cachés n'a aucun intérêt... Mais si on ne traite que le premier état observé, on n'a pas fait le travail complètement non plus...\n",
    "\n",
    "Complétons donc la solution précédente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Passage de l'ensemble de la séquence\n",
    "\n",
    "hidden = rnn.initHidden() # en dehors de la boucle\n",
    "for i in range(len(seq_x)):  # ca devrait marcher, le temps est stocké dans la première dim\n",
    "    yhat, hidden = rnn(seq_x[i,:,:], hidden) # iteration sur les états cachés calculés\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN V2\n",
    "\n",
    "La solution actuelle est insatisfaisante: le `forward` est non standard, on ne peut pas intégrer cette architecture dans notre boucle classique de train...\n",
    "On a donc besoin d'une nouvelle version.\n",
    "\n",
    "Intégrer la boucle de récurrence à l'interieur du forward pour avoir:\n",
    "- une séquence en entrée\n",
    "- une séquence en sortie (toutes les couches cachées) + une sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_V2, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size) # W2\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # W3\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden==None: hidden = self.initHidden() # init par défaut à 0\n",
    "        allh = []\n",
    "\n",
    "        # calculer tous les états cachés\n",
    "        # les ajouter dans la liste\n",
    "        # Former un tensor avec tous les éléments de la liste (torch.cat)\n",
    "        ##  TODO \n",
    "      \n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, allh\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier les dimensions attendues dans la boite ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([23, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN_V2(d_data, n_hidden, n_cl)\n",
    "\n",
    "yhat, hidden = rnn(seq_x)\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
