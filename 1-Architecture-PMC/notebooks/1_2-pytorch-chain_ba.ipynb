{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de résoudre les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "# A. Exemple typique de code complet & applications\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework.\n",
    "\n",
    "## A.1. Exemple complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "b3TRg2p5ldCJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# pour les MAC\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir c:\\tmp\\logs\n",
      "Une fois effectué, copier-coller l'URL dans votre navigateur pour avoir les courbes d'apprentissage\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# outils avancés de gestion des chemins\n",
    "BASEPATH = Path(\"/tmp\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois effectué, copier-coller l'URL dans votre navigateur pour avoir les courbes d'apprentissage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets: construction du jeu de données + séparation apprentissage / test\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(data_home=\"./data/\") ## chargement des données\n",
    "all_data = torch.tensor(housing['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "      else:\n",
    "           print(\"Erreur de chargement du fichier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La boite suivante prend 3 ou 4 minutes\n",
    "* Utiliser TensorBoard pour suivre l'apprentissage\n",
    "* pas besoin d'aller au bout: ce qui nous intéresse, c'est l'architecture générale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau\"\n",
    "net = net.to(device)\n",
    "MyLoss = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=1e-5)\n",
    "#optim = torch.optim.Adam(params=net.parameters(),lr=1e-3)\n",
    "\n",
    "# On créé un writer avec la date du modèle pour s'y retrouver\n",
    "log_dir = Path(f\"./tmp/logs/model-{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary = SummaryWriter(log_dir)\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Apprentissage\n",
    "    # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "    net.train()\n",
    "    cumloss = 0\n",
    "    for xbatch, ybatch in train_loader:\n",
    "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "        outputs = net(xbatch)\n",
    "        loss = MyLoss(outputs.view(-1),ybatch)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    summary.add_scalar(\"loss/train loss\",  cumloss/len(train_loader),epoch)\n",
    "     \n",
    "    if epoch % 10 == 0: \n",
    "        # Validation\n",
    "        # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            cumloss = 0\n",
    "            for xbatch, ybatch in valid_loader:\n",
    "                xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                outputs = net(xbatch)\n",
    "                cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation loss\", cumloss/len(valid_loader) ,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si vous êtes allé au bout de l'entrainement\n",
    "path = Path(\"./model\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "fichier = path/f\"{net.name}\"\n",
    "\n",
    "# ramener le réseau en mémoire avant la sauvegarde\n",
    "save_model(net.to(\"cpu\"),fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger un modèle déjà entrainé (attention, écrasement du réseau présent en mémoire)\n",
    "\n",
    "load_model(\"model/premier-reseau-pretrained\", net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  32.86219432204962\n"
     ]
    }
   ],
   "source": [
    "# test de la performance du réseau chargé:\n",
    "with torch.no_grad():\n",
    "    cumloss = 0\n",
    "    for xbatch, ybatch in valid_loader:\n",
    "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "        outputs = net(xbatch)\n",
    "        cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
    "print(\"loss: \",cumloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HujGOuB9lte2"
   },
   "source": [
    "## A.2. Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "KH_GScQbltD0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "# Téléchargement des données\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = train_set[0][0].shape[1],train_set[0][0].shape[2] # taille de l'image\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\n",
    "all_train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "all_test_loader = DataLoader(test_set,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "alloOoFulri7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## Affichage de quelques chiffres\n",
    "ex,lab = next(iter(all_train_loader))\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(ex[i].view(WIDTH,HEIGHT), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Label : {}\".format(lab[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  ax = plt.gca()\n",
    "  ax.set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjPoDcEj28uk"
   },
   "source": [
    "##  A.3. <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couche de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$Sotfmax(\\mathbf{x}) = \\frac{\\exp{x_i}}{\\sum_{i=1^d} x_i}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y}$$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "38Q58_1b-tQs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(train_set)*TRAIN_RATIO)\n",
    "ds_train, ds_test = random_split(train_set, (train_length, len(train_set)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    # si  y encode les indexes\n",
    "    if len(y.shape)==1 or y.size(1)==1:\n",
    "        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()\n",
    "    # si y est encodé en onehot\n",
    "    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 6.0), (2.0, 12.0), (3.0, 22.0)]\n"
     ]
    }
   ],
   "source": [
    "# DIGRESSION : mini tuto sur les fonctions lambda\n",
    "\n",
    "# 1. Construire la fonction 2 x^2 + 4\n",
    "monpoly = lambda x : 2 * x**2 + 4\n",
    "\n",
    "# 2. Utiliser la fonction\n",
    "x = [1., 2., 3.]\n",
    "print([(i, monpoly(i)) for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "Une erreur s'est produite : one_hot(): argument 'input' (position 1) must be Tensor, not int\n"
     ]
    }
   ],
   "source": [
    "# 3. Autre exemple pour transformer un entier en vecteur one-hot de dimension 3\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "to_one_hot = lambda x: one_hot(x,3).float()\n",
    "\n",
    "# on est dans torch => il faut donner des structures torch\n",
    "print(to_one_hot(torch.tensor(0)))\n",
    "print(to_one_hot(torch.tensor(1)))\n",
    "try:\n",
    "    print(to_one_hot(2))\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur s'est produite : {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On construit un réseau générique, où le nombre de couche pourra varier\n",
    "# 10 sorties pour chaque réseau, une par classe. \n",
    "# DANS la boucle d'apprentissage:\n",
    "#   SOL 1: cross entropy loss, 10 sorties, on ne rescale pas les sorties <=> GT = int\n",
    "#          (la cross entropy combine un softmax + NLLloss)\n",
    "#   SOL 2: MSE, 10 sorties + softmax <=> GT = 10 floats 0./1.\n",
    "\n",
    "# Etape 1: Réseau générique\n",
    "\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self,in_features,out_features,dims=[16],activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        # liste des couches\n",
    "        layers = []\n",
    "        # Remplir la liste avec les bons modules, à la bonne taille, dans l'ordre\n",
    "        ##  TODO \n",
    "        self.layers = nn.Sequential(*layers) # astuce pour créer le réseau à partir de la liste!\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)\n",
    "        \n",
    "# Boucle typique pour l'entraînement\n",
    "def run(model,epochs,loss_type=\"MSE\"):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/{model.name}-{loss_type}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3) # gradient le plus classique/robuste\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"running {model.name}--{loss_type}\")\n",
    "\n",
    "    # SI loss_type == MSE\n",
    "    #   - definition de la loss : MSELoss\n",
    "    #   - transformation des sorties du réseau : transf_out = softmax \n",
    "    #   - transformation des étiquettes : transf_lab = one_hot + passage en float\n",
    "    # SINON\n",
    "    #   - definition de la loss : CrossEntropyLoss\n",
    "    #   - transfos = identité\n",
    "\n",
    "    ##  TODO \n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), transf_lab(y).to(device) # usage des fonctions def ci-dessus\n",
    "            yhat = transf_out(model(x)) # usage des fonctions def ci-dessus\n",
    "            l = loss(yhat,y) # usage des fonctions def ci-dessus\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar(f'loss{loss_type}/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), transf_lab(y).to(device)\n",
    "                    yhat = transf_out(model(x))\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss{loss_type}/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] La syntaxe du nom de fichier, de répertoire ou de volume est incorrecte: '/tmp/logs/net-16-1Wed Oct 22 09:22:47 2025-MSE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nb_dim \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m16\u001b[39m,\u001b[32m64\u001b[39m]:\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Créer l'instance de réseau avec les bons paramètres\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m##  TODO \u001b[39;00m\n\u001b[32m     14\u001b[39m     net.name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnet-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_couches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m+time.asctime()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# ramener le réseau en mémoire avant la sauvegarde\u001b[39;00m\n\u001b[32m     18\u001b[39m     fichier = path/\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(model, epochs, loss_type)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(model,epochs,loss_type=\u001b[33m\"\u001b[39m\u001b[33mMSE\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     writer = \u001b[43mSummaryWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/tmp/logs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mloss_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     optim = torch.optim.Adam(model.parameters(),lr=\u001b[32m1e-3\u001b[39m) \u001b[38;5;66;03m# gradient le plus classique/robuste\u001b[39;00m\n\u001b[32m     26\u001b[39m     model = model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:249\u001b[39m, in \u001b[36mSummaryWriter.__init__\u001b[39m\u001b[34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Initialize the file writers, but they can be cleared out on close\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# and recreated later as needed.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m.file_writer = \u001b[38;5;28mself\u001b[39m.all_writers = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_file_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n\u001b[32m    252\u001b[39m v = \u001b[32m1e-12\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:281\u001b[39m, in \u001b[36mSummaryWriter._get_file_writer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the default FileWriter instance. Recreates it if closed.\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_writers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28mself\u001b[39m.file_writer = \u001b[43mFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename_suffix\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m.all_writers = {\u001b[38;5;28mself\u001b[39m.file_writer.get_logdir(): \u001b[38;5;28mself\u001b[39m.file_writer}\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.purge_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:75\u001b[39m, in \u001b[36mFileWriter.__init__\u001b[39m\u001b[34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Sometimes PosixPath is passed in and we need to coerce it to\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# a string in all cases\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# TODO: See if we can remove this in the future if we are\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# actually the ones passing in a PosixPath\u001b[39;00m\n\u001b[32m     74\u001b[39m log_dir = \u001b[38;5;28mstr\u001b[39m(log_dir)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28mself\u001b[39m.event_writer = \u001b[43mEventFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_suffix\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py:72\u001b[39m, in \u001b[36mEventFileWriter.__init__\u001b[39m\u001b[34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates a `EventFileWriter` and an event file to write to.\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33;03mOn construction the summary writer creates a new event file in `logdir`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m \u001b[33;03m    pending events and summaries to disk.\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m._logdir = logdir\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m._file_name = (\n\u001b[32m     74\u001b[39m     os.path.join(\n\u001b[32m     75\u001b[39m         logdir,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     + filename_suffix\n\u001b[32m     85\u001b[39m )  \u001b[38;5;66;03m# noqa E128\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m._general_file_writer = tf.io.gfile.GFile(\u001b[38;5;28mself\u001b[39m._file_name, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:909\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmakedirs\u001b[39m(path):\n\u001b[32m    902\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[32m    903\u001b[39m \n\u001b[32m    904\u001b[39m \u001b[33;03m    It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    907\u001b[39m \u001b[33;03m      path: string, name of the directory to be created\u001b[39;00m\n\u001b[32m    908\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_filesystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\Anaconda3\\envs\\torch2025\\Lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:208\u001b[39m, in \u001b[36mLocalFileSystem.makedirs\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmakedirs\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m    207\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a directory and all parent/intermediate directories.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 123] La syntaxe du nom de fichier, de répertoire ou de volume est incorrecte: '/tmp/logs/net-16-1Wed Oct 22 09:22:47 2025-MSE'"
     ]
    }
   ],
   "source": [
    "# on va tester 12 réseaux = 2 loss x 3 profondeurs x 2 nb de neurones cachés\n",
    "# on choisira une activation de type RELU\n",
    "\n",
    "# si vous êtes allé au bout de l'entrainement\n",
    "path = Path(\"./model\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for l in [\"MSE\",\"CEL\"]:\n",
    "    for nb_couches in [1,2,3]:\n",
    "        for nb_dim in [16,64]:\n",
    "\n",
    "            # Créer l'instance de réseau avec les bons paramètres\n",
    "            ##  TODO \n",
    "            net.name = f\"net-{nb_dim}-{nb_couches}\"+time.asctime()\n",
    "            run(net,2000,l)\n",
    "\n",
    "            # ramener le réseau en mémoire avant la sauvegarde\n",
    "            fichier = path/f\"{net.name}\"\n",
    "            save_model(net.to(\"cpu\"),fichier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.4.  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser (cf code ci-dessous).\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, observez les histogrammes de la distribution des poids et l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Utilisez pour cela  un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_l2(model,epochs,l2_coef):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/l2-{l2_coef}-{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}-{l2_coef}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "\n",
    "            # Ajout d'une pénalisation L2 sur toutes les couches\n",
    "            l2_loss = 0.\n",
    "            for name, value in model.named_parameters():\n",
    "                if name.endswith(\".weight\"):\n",
    "                    l2_loss += (value ** 2).sum()\n",
    "            l += l2_coef*l2_loss\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "            \n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        writer.add_scalar('loss/l2',l2_loss,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                ix = 0\n",
    "                for module in model.layers:\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "                        ix += 1\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQsK7jHuxKqM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_dropout(model,epochs):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "\n",
    "\n",
    "def get_dropout_net(in_features,out_features,dims,dropout):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    \n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        if dropout>0: layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batchnorm_net(in_features,out_features,dims):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        layers.append(nn.BatchNorm1d(dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
